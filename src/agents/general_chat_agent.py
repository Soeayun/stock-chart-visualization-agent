"""
General Chat Agent: ì°¨íŠ¸ì™€ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” ì²˜ë¦¬
"""
from typing import Dict, Any
from langchain.chat_models import init_chat_model
from langgraph.store.base import BaseStore
from ..schemas import State
from ..prompts import GENERAL_CHAT_SYSTEM_PROMPT, GENERAL_CHAT_USER_PROMPT


def get_conversation_history(store: BaseStore, namespace: tuple) -> str:
    """Storeì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # Storeì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê²€ìƒ‰
        memories = store.search(namespace)
        if memories:
            # ê°€ì¥ ìµœê·¼ ë©”ëª¨ë¦¬ì˜ ë‚´ìš© ë°˜í™˜
            return memories[-1].value
        return "ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—†ìŒ"
    except:
        return "ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—†ìŒ"


def update_conversation_history(store: BaseStore, namespace: tuple, user_message: str, ai_response: str):
    """Storeì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
    import uuid
    
    # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    existing_history = get_conversation_history(store, namespace)
    
    # ìƒˆë¡œìš´ ëŒ€í™” ì¶”ê°€
    new_entry = f"ì‚¬ìš©ì: {user_message}\nAI: {ai_response}"
    updated_history = f"{existing_history}\n{new_entry}" if existing_history != "ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—†ìŒ" else new_entry
    
    # Storeì— ì €ì¥
    memory_id = str(uuid.uuid4())
    store.put(namespace, memory_id, {"conversation_history": updated_history})


def general_chat_agent(state: State, store: BaseStore) -> State:
    """
    ì¼ë°˜ ëŒ€í™” Agent: ì°¨íŠ¸ì™€ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” ì²˜ë¦¬
    - ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€
    - ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ì°¨íŠ¸ ì‹œê°í™” ì œì•ˆ
    - Storeë¥¼ í†µí•œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    """
    print("ğŸ’¬ General Chat Agent ì‹¤í–‰ ì¤‘...")
    
    # LLM ì´ˆê¸°í™”
    llm = init_chat_model("openai:gpt-4o", temperature=0.1)
    
    # Storeì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    namespace = ("stock_chart_agent", "conversation_history")
    conversation_history = get_conversation_history(store, namespace)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = GENERAL_CHAT_USER_PROMPT.format(
        conversation_history=conversation_history,
        user_message=state["user_message"]
    )
    
    # LLM í˜¸ì¶œ
    response = llm.invoke([
        {"role": "system", "content": GENERAL_CHAT_SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ])
    
    # Storeì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    update_conversation_history(store, namespace, state["user_message"], response.content)
    
    print(f"ì‘ë‹µ: {response.content}")
    
    return {
        "chart_output": response.content
    }
