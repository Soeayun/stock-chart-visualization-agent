"""
Router Agent: ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì°¨íŠ¸ ìš”ì²­ ì˜ë„ë¥¼ íŒë³„
"""
from typing import Dict, Any
from langchain.chat_models import init_chat_model
from ..schemas import State, RouterSchema
from ..prompts import ROUTER_SYSTEM_PROMPT, ROUTER_USER_PROMPT


def router_agent(state: State) -> State:
    """
    Router Agent: ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì°¨íŠ¸ ìš”ì²­ ì˜ë„ë¥¼ íŒë³„
    - ì°¨íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„
    - ì¼ë°˜ ëŒ€í™” vs ì°¨íŠ¸ ìš”ì²­ êµ¬ë¶„
    """
    print("ğŸ”€ Router Agent ì‹¤í–‰ ì¤‘...")
    
    # LLM ì´ˆê¸°í™”
    llm = init_chat_model("openai:gpt-4o", temperature=0.0)
    llm_router = llm.with_structured_output(RouterSchema) # llmì´ ì¶œë ¥ê°’ì„ í•´ë‹¹ schemaì— ë§ê²Œ ë°˜í™˜í•˜ë„ë¡ í•˜ëŠ” ë‚´ì¥ method
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = ROUTER_USER_PROMPT.format(user_message=state["user_message"])
    
    # LLM í˜¸ì¶œ
    result = llm_router.invoke([
        {"role": "system", "content": ROUTER_SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ])
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ë¶„ë¥˜ ê·¼ê±°: {result.reasoning}")
    print(f"ë¶„ë¥˜ ê²°ê³¼: {result.is_chart_request}")
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    return {
        "is_chart_request": result.is_chart_request,
        "routing_decision": "chart_request" if result.is_chart_request else "general_chat"
    }
